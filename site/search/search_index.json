{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Index"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"about/","text":"","title":"About"},{"location":"setup/hadoop-single-node-installation/","text":"\u5355\u8282\u70b9Hadoop\u955c\u50cf \u4ee5Dockerfile\u7684\u65b9\u5f0f\uff0c\u5728Ubuntu 18.04\u4e0a\u5b89\u88c5\u5355\u8282\u70b9\u7684Hadoop 3.3.0\u96c6\u7fa4\u3002 \u83b7\u53d6\u73b0\u6210\u955c\u50cf \u672c\u6587\u6863\u4e2d\u6784\u5efa\u7684\u955c\u50cf\u5df2\u7ecf\u4e0a\u4f20\u5728\u81f3DockerHub\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528docker\u547d\u4ee4\u62c9\u53d6\u5e76\u8fd0\u884c\uff1a docker run -h bigdata -p 8088:8088 -it iamabug1128/hadoop bash \u6ce8\u610f\uff1a\u8fd9\u91cc\u7684-h bigdata\u662f\u5fc5\u987b\u7684\uff0c\u56e0\u4e3ahadoop\u7684\u914d\u7f6e\u6587\u4ef6\u4f9d\u8d56\u4e8e\u8fd9\u4e2a\u7279\u5b9a\u7684\u4e3b\u673a\u540d\u3002 8088\u7aef\u53e3\u7684\u6620\u5c04\u662f\u4e3a\u4e86\u65b9\u4fbf\u5728\u5bb9\u5668\u5916\u8bbf\u95eeYARN\u7684Web UI\u3002 \u8fd9\u4e2a\u955c\u50cf\u5bf9\u5e94\u7684Dockerfile\u53ef\u4ee5\u5728github\u4e0a\u627e\u5230\uff1a https://github.com/iamabug/bdt/tree/main/code/dockerfiles/hadoop \uff0c\u6839\u636e\u9700\u8981\u8fdb\u884c\u8c03\u6574\u3002 \u5b89\u88c5\u914d\u7f6eOpenJDK Hadoop\u751f\u6001\u4e2d\u7684\u8f6f\u4ef6\u5927\u90fd\u4f7f\u7528Java/Scala\u5f00\u53d1\uff0cJava\u8fd0\u884c\u73af\u5883\u662f\u5fc5\u9700\u7684\u3002 RUN apt-get update && apt-get install -y openjdk-8-jdk ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \u5b89\u88c5\u914d\u7f6eOpenSSH \u5728Hadoop\u5e38\u89c1\u7684\u64cd\u4f5c\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u80fd\u591f\u5728\u4e00\u4e2a\u8282\u70b9\uff08\u6bd4\u5982NameNode\uff09\u4e0a\u63a7\u5236\u5176\u4f59\u8282\u70b9\u4e0a\u7684Hadoop\u670d\u52a1\uff0c\u56e0\u6b64\u9700\u8981\u5b89\u88c5SSH\u670d\u52a1\uff0c\u5e76\u4e14\u914d\u7f6e\u5bc6\u94a5\u767b\u5f55\u7684\u65b9\u5f0f\uff0c\u7b80\u5316\u64cd\u4f5c\u3002 \u5b89\u88c5\uff1a RUN apt-get install -y openssh-server openssh-client \u914d\u7f6e\uff1a # \u6dfb\u52a0hadoop\u7ec4\u548c\u7528\u6237\uff0c\u4f7f\u7528hadoop\u7528\u6237\u7ba1\u7406hadoop\u670d\u52a1 RUN addgroup hadoop RUN adduser --ingroup hadoop --quiet --disabled-password hadoop # \u4e3ahadoop\u7528\u6237\u751f\u6210\u516c\u79c1\u94a5 RUN su hadoop -c \"ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && chmod 0600 ~/.ssh/authorized_keys\" \u5b89\u88c5\u914d\u7f6eHadoop \u4ece\u56fd\u5185\u955c\u50cf\u4e0b\u8f7dHadoop\u4e8c\u8fdb\u5236\u5305\uff1a wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz -O hadoop.tar.gz \u6dfb\u52a0\u5230\u955c\u50cf\u4e2d\uff1a # ADD \u4f1a\u81ea\u52a8\u8fdb\u884c\u89e3\u538b ADD hadoop.tar.gz /usr/local RUN mv /usr/local/hadoop-3.3.0 /usr/local/hadoop # \u521b\u5efa\u914d\u7f6e\u6587\u4ef6\u76ee\u5f55\u548c\u6570\u636e\u76ee\u5f55 RUN ln -s /usr/local/hadoop/etc/hadoop /etc/hadoop RUN mkdir -p /usr/local/hadoop/data/{namenode,datanode} /etc/hadoop-httpfs/conf/ /usr/local/hadoop/logs \u4fee\u6539\u914d\u7f6e\u548c\u76ee\u5f55\u6743\u9650\uff1a RUN echo \"export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\" >> /etc/hadoop/hadoop-env.sh && \\ echo \"bigdata\" > /etc/hadoop/workers && \\ chown -R hadoop:hadoop /usr/local/hadoop \u62f7\u8d1d\u914d\u7f6e\u6587\u4ef6\uff1a COPY core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml /etc/hadoop/ \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\uff1a # PATH\u73af\u5883\u53d8\u91cf\uff0c\u8fd9\u91cc\u8bbe\u7f6e\u4e86\u4e24\u6b21\uff0cENV\u7684\u65b9\u6cd5\u53ea\u5bf9root\u7528\u6237\u751f\u6548\uff0c/etc/environment\u5bf9\u5176\u5b83\u7528\u6237\u751f\u6548 ENV PATH=/usr/local/hadoop/bin:/usr/local/hadoop/sbin:${PATH} RUN echo \"PATH=/usr/local/hadoop/bin:/usr/local/hadoop/sbin:${PATH}\" >> /etc/environment \u914d\u7f6e\u6587\u4ef6\u8bf4\u660e \u4e3a\u4e86\u8ba9HDFS\u3001MapReduce\u548cYARN\u53ef\u4ee5\u6b63\u5e38\u8fd0\u884c\uff0cHadoop\u7684\u5404\u4e2a\u914d\u7f6e\u6587\u4ef6\u9700\u8981\u4e00\u4e9b\u57fa\u7840\u7684\u914d\u7f6e\u9879\u3002 core-site.xml <configuration> <property> <name>fs.defaultFS</name> <!--\u8fd9\u91cc\u5047\u8bbe\u5bb9\u5668\u4e3b\u673a\u540d\u4e3abigdata--> <value>hdfs://bigdata:8020/</value> <description>NameNode URI</description> </property> </configuration> hdfs-site.xml <configuration> <!--\u56e0\u4e3a\u662f\u5355\u8282\u70b9\u7684\u96c6\u7fa4\uff0c\u6240\u4ee5hdfs\u7684\u526f\u672c\u6570\u8bbe\u4e3a1--> <property> <name>dfs.replication</name> <value>1</value> </property> <!--\u6307\u5b9anamenode\u6570\u636e\u76ee\u5f55--> <property> <name>dfs.name.dir</name> <value>/usr/local/hadoop/data/namenode</value> </property> <!--\u6307\u5b9adatanode\u6570\u636e\u6a21\u6d41--> <property> <name>dfs.data.dir</name> <value>/usr/local/hadoop/data/datanode</value> </property> <!--\u542f\u7528webhdfs\uff0c\u53ef\u9009--> <property> <name>dfs.webhdfs.enable</name> <value>true</value> </property> </configuration> mapred-site.xml <configuration> <!--\u5728YARN\u4e0a\u8fd0\u884cMapReduce\u4efb\u52a1--> <property> <name>mapreduce.framework.name</name> <value>yarn</value> </property> <!--\u4e0b\u9762\u4e09\u4e2a\u914d\u7f6e\u9879\u5982\u679c\u4e0d\u8bbe\u7f6e\uff0c\u8fd0\u884cMR\u4efb\u52a1\u65f6\u4f1a\u63d0\u793a\u5e76\u62a5\u9519--> <property> <name>yarn.app.mapreduce.am.env</name> <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value> </property> <property> <name>mapreduce.map.env</name> <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value> </property> <property> <name>mapreduce.reduce.env</name> <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value> </property> </configuration> yarn-site.xml <configuration> <property> <!--\u6ca1\u6709\u6b64\u914d\u7f6e\u8fd0\u884c\u4e0b\u6587\u6240\u8ff0example\u4efb\u52a1\u4f1a\u62a5\u9519--> <name>yarn.nodemanager.aux-services</name> <value>mapreduce_shuffle</value> </property> </configuration> \u542f\u52a8\u670d\u52a1 \u5148\u542f\u52a8ssh\u670d\u52a1\uff1a service ssh start \u7136\u540e\u4ee5hadoop\u7528\u6237\u8eab\u4efd\u542f\u52a8hadoop\u7684\u6240\u6709\u670d\u52a1\uff1a su hadoop -c \"hdfs namenode -format && start-all.sh\" \u542f\u52a8\u540e\u67e5\u770b\u8fdb\u7a0b\uff1a jps 512 DataNode 401 NameNode 1667 Jps 1108 NodeManager 743 SecondaryNameNode 990 ResourceManager \u53ef\u4ee5\u770b\u5230\uff0c\u6210\u529f\u7684\u542f\u52a8\u4e86DataNode\u3001NameNode\u3001SecondaryNameNode\u3001NodeManager\u3001ResourceManager\u3002 \u529f\u80fd\u6d4b\u8bd5 \u521b\u5efa\u76ee\u5f55\uff1a su hadoop hdfs dfs -mkdir -p /user/hadoop/input \u4e0a\u4f20hadoop\u7684xml\u914d\u7f6e\u6587\u4ef6\u5230HDFS\uff1a hdfs dfs -put /etc/hadoop/*.xml /user/hadoop/input \u8fd0\u884chadoop\u81ea\u5e26\u7684\u4e00\u4e2a\u5b57\u7b26\u4e32\u68c0\u7d22\u4efb\u52a1\uff0c\u627e\u51fa\u6240\u6709dfs\u5f00\u5934\u7684\u5b57\u7b26\u4e32\uff1a hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar grep input output 'dfs[a-z.]+' \u63d0\u4ea4\u4efb\u52a1\u4e4b\u540e\uff0c\u5728\u5bbf\u4e3b\u673a\u7684\u6d4f\u89c8\u5668\u4e2d\u8bbf\u95ee http://localhost:8088 \uff0c\u53ef\u4ee5\u770b\u5230 grep-search \u4efb\u52a1\u7684\u4fe1\u606f\uff1a \u5f85\u4efb\u52a1\u8fd0\u884c\u5b8c\u6bd5\u540e\uff0c\u53ef\u4ee5\u67e5\u770b\u8f93\u51fa\uff1a hdfs dfs -text /user/hadoop/output/* 1 dfsadmin 1 dfs.webhdfs.enable 1 dfs.replication 1 dfs.name.dir 1 dfs.data.dir \u53c2\u8003\u6587\u6863 https://haadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html","title":"Hadoop \u5355\u673a\u7248\u5b89\u88c5"},{"location":"setup/hadoop-single-node-installation/#hadoop","text":"\u4ee5Dockerfile\u7684\u65b9\u5f0f\uff0c\u5728Ubuntu 18.04\u4e0a\u5b89\u88c5\u5355\u8282\u70b9\u7684Hadoop 3.3.0\u96c6\u7fa4\u3002","title":"\u5355\u8282\u70b9Hadoop\u955c\u50cf"},{"location":"setup/hadoop-single-node-installation/#_1","text":"\u672c\u6587\u6863\u4e2d\u6784\u5efa\u7684\u955c\u50cf\u5df2\u7ecf\u4e0a\u4f20\u5728\u81f3DockerHub\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528docker\u547d\u4ee4\u62c9\u53d6\u5e76\u8fd0\u884c\uff1a docker run -h bigdata -p 8088:8088 -it iamabug1128/hadoop bash \u6ce8\u610f\uff1a\u8fd9\u91cc\u7684-h bigdata\u662f\u5fc5\u987b\u7684\uff0c\u56e0\u4e3ahadoop\u7684\u914d\u7f6e\u6587\u4ef6\u4f9d\u8d56\u4e8e\u8fd9\u4e2a\u7279\u5b9a\u7684\u4e3b\u673a\u540d\u3002 8088\u7aef\u53e3\u7684\u6620\u5c04\u662f\u4e3a\u4e86\u65b9\u4fbf\u5728\u5bb9\u5668\u5916\u8bbf\u95eeYARN\u7684Web UI\u3002 \u8fd9\u4e2a\u955c\u50cf\u5bf9\u5e94\u7684Dockerfile\u53ef\u4ee5\u5728github\u4e0a\u627e\u5230\uff1a https://github.com/iamabug/bdt/tree/main/code/dockerfiles/hadoop \uff0c\u6839\u636e\u9700\u8981\u8fdb\u884c\u8c03\u6574\u3002","title":"\u83b7\u53d6\u73b0\u6210\u955c\u50cf"},{"location":"setup/hadoop-single-node-installation/#openjdk","text":"Hadoop\u751f\u6001\u4e2d\u7684\u8f6f\u4ef6\u5927\u90fd\u4f7f\u7528Java/Scala\u5f00\u53d1\uff0cJava\u8fd0\u884c\u73af\u5883\u662f\u5fc5\u9700\u7684\u3002 RUN apt-get update && apt-get install -y openjdk-8-jdk ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64","title":"\u5b89\u88c5\u914d\u7f6eOpenJDK"},{"location":"setup/hadoop-single-node-installation/#openssh","text":"\u5728Hadoop\u5e38\u89c1\u7684\u64cd\u4f5c\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u80fd\u591f\u5728\u4e00\u4e2a\u8282\u70b9\uff08\u6bd4\u5982NameNode\uff09\u4e0a\u63a7\u5236\u5176\u4f59\u8282\u70b9\u4e0a\u7684Hadoop\u670d\u52a1\uff0c\u56e0\u6b64\u9700\u8981\u5b89\u88c5SSH\u670d\u52a1\uff0c\u5e76\u4e14\u914d\u7f6e\u5bc6\u94a5\u767b\u5f55\u7684\u65b9\u5f0f\uff0c\u7b80\u5316\u64cd\u4f5c\u3002 \u5b89\u88c5\uff1a RUN apt-get install -y openssh-server openssh-client \u914d\u7f6e\uff1a # \u6dfb\u52a0hadoop\u7ec4\u548c\u7528\u6237\uff0c\u4f7f\u7528hadoop\u7528\u6237\u7ba1\u7406hadoop\u670d\u52a1 RUN addgroup hadoop RUN adduser --ingroup hadoop --quiet --disabled-password hadoop # \u4e3ahadoop\u7528\u6237\u751f\u6210\u516c\u79c1\u94a5 RUN su hadoop -c \"ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && chmod 0600 ~/.ssh/authorized_keys\"","title":"\u5b89\u88c5\u914d\u7f6eOpenSSH"},{"location":"setup/hadoop-single-node-installation/#hadoop_1","text":"\u4ece\u56fd\u5185\u955c\u50cf\u4e0b\u8f7dHadoop\u4e8c\u8fdb\u5236\u5305\uff1a wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz -O hadoop.tar.gz \u6dfb\u52a0\u5230\u955c\u50cf\u4e2d\uff1a # ADD \u4f1a\u81ea\u52a8\u8fdb\u884c\u89e3\u538b ADD hadoop.tar.gz /usr/local RUN mv /usr/local/hadoop-3.3.0 /usr/local/hadoop # \u521b\u5efa\u914d\u7f6e\u6587\u4ef6\u76ee\u5f55\u548c\u6570\u636e\u76ee\u5f55 RUN ln -s /usr/local/hadoop/etc/hadoop /etc/hadoop RUN mkdir -p /usr/local/hadoop/data/{namenode,datanode} /etc/hadoop-httpfs/conf/ /usr/local/hadoop/logs \u4fee\u6539\u914d\u7f6e\u548c\u76ee\u5f55\u6743\u9650\uff1a RUN echo \"export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\" >> /etc/hadoop/hadoop-env.sh && \\ echo \"bigdata\" > /etc/hadoop/workers && \\ chown -R hadoop:hadoop /usr/local/hadoop \u62f7\u8d1d\u914d\u7f6e\u6587\u4ef6\uff1a COPY core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml /etc/hadoop/ \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\uff1a # PATH\u73af\u5883\u53d8\u91cf\uff0c\u8fd9\u91cc\u8bbe\u7f6e\u4e86\u4e24\u6b21\uff0cENV\u7684\u65b9\u6cd5\u53ea\u5bf9root\u7528\u6237\u751f\u6548\uff0c/etc/environment\u5bf9\u5176\u5b83\u7528\u6237\u751f\u6548 ENV PATH=/usr/local/hadoop/bin:/usr/local/hadoop/sbin:${PATH} RUN echo \"PATH=/usr/local/hadoop/bin:/usr/local/hadoop/sbin:${PATH}\" >> /etc/environment","title":"\u5b89\u88c5\u914d\u7f6eHadoop"},{"location":"setup/hadoop-single-node-installation/#_2","text":"\u4e3a\u4e86\u8ba9HDFS\u3001MapReduce\u548cYARN\u53ef\u4ee5\u6b63\u5e38\u8fd0\u884c\uff0cHadoop\u7684\u5404\u4e2a\u914d\u7f6e\u6587\u4ef6\u9700\u8981\u4e00\u4e9b\u57fa\u7840\u7684\u914d\u7f6e\u9879\u3002","title":"\u914d\u7f6e\u6587\u4ef6\u8bf4\u660e"},{"location":"setup/hadoop-single-node-installation/#core-sitexml","text":"<configuration> <property> <name>fs.defaultFS</name> <!--\u8fd9\u91cc\u5047\u8bbe\u5bb9\u5668\u4e3b\u673a\u540d\u4e3abigdata--> <value>hdfs://bigdata:8020/</value> <description>NameNode URI</description> </property> </configuration>","title":"core-site.xml"},{"location":"setup/hadoop-single-node-installation/#hdfs-sitexml","text":"<configuration> <!--\u56e0\u4e3a\u662f\u5355\u8282\u70b9\u7684\u96c6\u7fa4\uff0c\u6240\u4ee5hdfs\u7684\u526f\u672c\u6570\u8bbe\u4e3a1--> <property> <name>dfs.replication</name> <value>1</value> </property> <!--\u6307\u5b9anamenode\u6570\u636e\u76ee\u5f55--> <property> <name>dfs.name.dir</name> <value>/usr/local/hadoop/data/namenode</value> </property> <!--\u6307\u5b9adatanode\u6570\u636e\u6a21\u6d41--> <property> <name>dfs.data.dir</name> <value>/usr/local/hadoop/data/datanode</value> </property> <!--\u542f\u7528webhdfs\uff0c\u53ef\u9009--> <property> <name>dfs.webhdfs.enable</name> <value>true</value> </property> </configuration>","title":"hdfs-site.xml"},{"location":"setup/hadoop-single-node-installation/#mapred-sitexml","text":"<configuration> <!--\u5728YARN\u4e0a\u8fd0\u884cMapReduce\u4efb\u52a1--> <property> <name>mapreduce.framework.name</name> <value>yarn</value> </property> <!--\u4e0b\u9762\u4e09\u4e2a\u914d\u7f6e\u9879\u5982\u679c\u4e0d\u8bbe\u7f6e\uff0c\u8fd0\u884cMR\u4efb\u52a1\u65f6\u4f1a\u63d0\u793a\u5e76\u62a5\u9519--> <property> <name>yarn.app.mapreduce.am.env</name> <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value> </property> <property> <name>mapreduce.map.env</name> <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value> </property> <property> <name>mapreduce.reduce.env</name> <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value> </property> </configuration>","title":"mapred-site.xml"},{"location":"setup/hadoop-single-node-installation/#yarn-sitexml","text":"<configuration> <property> <!--\u6ca1\u6709\u6b64\u914d\u7f6e\u8fd0\u884c\u4e0b\u6587\u6240\u8ff0example\u4efb\u52a1\u4f1a\u62a5\u9519--> <name>yarn.nodemanager.aux-services</name> <value>mapreduce_shuffle</value> </property> </configuration>","title":"yarn-site.xml"},{"location":"setup/hadoop-single-node-installation/#_3","text":"\u5148\u542f\u52a8ssh\u670d\u52a1\uff1a service ssh start \u7136\u540e\u4ee5hadoop\u7528\u6237\u8eab\u4efd\u542f\u52a8hadoop\u7684\u6240\u6709\u670d\u52a1\uff1a su hadoop -c \"hdfs namenode -format && start-all.sh\" \u542f\u52a8\u540e\u67e5\u770b\u8fdb\u7a0b\uff1a jps 512 DataNode 401 NameNode 1667 Jps 1108 NodeManager 743 SecondaryNameNode 990 ResourceManager \u53ef\u4ee5\u770b\u5230\uff0c\u6210\u529f\u7684\u542f\u52a8\u4e86DataNode\u3001NameNode\u3001SecondaryNameNode\u3001NodeManager\u3001ResourceManager\u3002","title":"\u542f\u52a8\u670d\u52a1"},{"location":"setup/hadoop-single-node-installation/#_4","text":"\u521b\u5efa\u76ee\u5f55\uff1a su hadoop hdfs dfs -mkdir -p /user/hadoop/input \u4e0a\u4f20hadoop\u7684xml\u914d\u7f6e\u6587\u4ef6\u5230HDFS\uff1a hdfs dfs -put /etc/hadoop/*.xml /user/hadoop/input \u8fd0\u884chadoop\u81ea\u5e26\u7684\u4e00\u4e2a\u5b57\u7b26\u4e32\u68c0\u7d22\u4efb\u52a1\uff0c\u627e\u51fa\u6240\u6709dfs\u5f00\u5934\u7684\u5b57\u7b26\u4e32\uff1a hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar grep input output 'dfs[a-z.]+' \u63d0\u4ea4\u4efb\u52a1\u4e4b\u540e\uff0c\u5728\u5bbf\u4e3b\u673a\u7684\u6d4f\u89c8\u5668\u4e2d\u8bbf\u95ee http://localhost:8088 \uff0c\u53ef\u4ee5\u770b\u5230 grep-search \u4efb\u52a1\u7684\u4fe1\u606f\uff1a \u5f85\u4efb\u52a1\u8fd0\u884c\u5b8c\u6bd5\u540e\uff0c\u53ef\u4ee5\u67e5\u770b\u8f93\u51fa\uff1a hdfs dfs -text /user/hadoop/output/* 1 dfsadmin 1 dfs.webhdfs.enable 1 dfs.replication 1 dfs.name.dir 1 dfs.data.dir","title":"\u529f\u80fd\u6d4b\u8bd5"},{"location":"setup/hadoop-single-node-installation/#_5","text":"https://haadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html","title":"\u53c2\u8003\u6587\u6863"}]}